1.EM算法:
    EM算法（Expectation-Maximization, 期望最大化算法）: 是一种用于在含有隐变量的统计模型中进行参数估计的迭代算法
    它尤其适合处理缺失数据或包含潜在变量（隐变量）的情形。该算法的基本思想是通过交替执行两个步骤，不断提高模型的似然函数值，
    从而得到模型参数的最大似然估计或最大后验估计

    核心步骤:
        E步（期望步骤，Expectation step）: 根据当前的模型参数，计算在给定观测数据下隐变量的期望（或分布），即计算隐变量的期望值。
                                       这一步的核心是生成当前模型的隐变量条件分布
        M步（最大化步骤，Maximization step）:
             在E步得到的隐变量期望的基础上，最大化似然函数或后验概率，更新模型参数。这一步是通过参数估计来最大化期望的对数似然函数
             这两个步骤会交替进行，直到模型参数收敛到某个稳定值

    EM算法的常见应用:
        高斯混合模型（Gaussian Mixture Model, GMM）：用来聚类数据，EM算法可以用来估计高斯分布的参数
        隐马尔可夫模型（Hidden Markov Model, HMM）：用于时间序列分析中的参数估计
        离散概率模型：如朴素贝叶斯分类器中，EM可以处理分类器的训练数据包含缺失值的情况
    优点:
        可以处理包含隐藏数据或缺失数据的复杂模型，逐步提高似然函数值，适合很多复杂的概率模型
    缺点:
        可能会收敛到局部最优解，依赖于初始参数的选择
        计算开销较大，特别是当数据规模很大时，收敛速度可能比较慢
        简单总结，EM算法是一个通过迭代计算期望值和最大化模型参数来处理隐含变量问题的强大工具，常用于机器学习和统计学中的各种模型优化问题
