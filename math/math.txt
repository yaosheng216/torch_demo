1.EM算法:
    EM算法（Expectation-Maximization, 期望最大化算法）: 是一种用于在含有隐变量的统计模型中进行参数估计的迭代算法
    它尤其适合处理缺失数据或包含潜在变量（隐变量）的情形。该算法的基本思想是通过交替执行两个步骤，不断提高模型的似然函数值，
    从而得到模型参数的最大似然估计或最大后验估计

    核心步骤:
        E步（期望步骤，Expectation step）: 根据当前的模型参数，计算在给定观测数据下隐变量的期望（或分布），即计算隐变量的期望值。
                                       这一步的核心是生成当前模型的隐变量条件分布
        M步（最大化步骤，Maximization step）:
             在E步得到的隐变量期望的基础上，最大化似然函数或后验概率，更新模型参数。这一步是通过参数估计来最大化期望的对数似然函数
             这两个步骤会交替进行，直到模型参数收敛到某个稳定值

    EM算法的常见应用:
        高斯混合模型（Gaussian Mixture Model, GMM）：用来聚类数据，EM算法可以用来估计高斯分布的参数
        隐马尔可夫模型（Hidden Markov Model, HMM）：用于时间序列分析中的参数估计
        离散概率模型：如朴素贝叶斯分类器中，EM可以处理分类器的训练数据包含缺失值的情况
    优点:
        可以处理包含隐藏数据或缺失数据的复杂模型，逐步提高似然函数值，适合很多复杂的概率模型
    缺点:
        可能会收敛到局部最优解，依赖于初始参数的选择
        计算开销较大，特别是当数据规模很大时，收敛速度可能比较慢
        简单总结，EM算法是一个通过迭代计算期望值和最大化模型参数来处理隐含变量问题的强大工具，常用于机器学习和统计学中的各种模型优化问题

2.朴素贝叶斯:
    朴素贝叶斯模型（Naive Bayes Model）是一种基于贝叶斯定理的简单而强大的分类算法，广泛应用于文本分类、垃圾邮件过滤、情感分析等领域。
    它之所以被称为“朴素”，是因为它假设特征之间是条件独立的，这在实际中通常是不成立的，但这种假设使得模型计算和实现都非常简单

    贝叶斯定理: P(A|B) = (P(B|A) * P(A)) / P(B)
              其中，P(A∣B) 是在事件B发生的条件下事件A发生的概率（后验概率）
              P(B∣A) 是在事件A发生的条件下事件B发生的概率（似然）
              P(A) 是事件A发生的先验概率
              P(B) 是事件B发生的先验概率

    朴素贝叶斯分类器:
        朴素贝叶斯分类器利用贝叶斯定理计算每个类别 Ck 的后验概率 P(Ck x)，其中x = (x1, x2, x3 ... xn)是输入特征向量，然后选择后验概率
        大的类别作为预测结果:
                C = arg max P(Ck | x)

    高斯朴素贝叶斯
    多项式朴素贝叶斯
    伯努利不俗贝叶斯

    优点:
        简单且易于实现
        计算效率高，适用于大规模数据
        对缺失数据不敏感
        在某些实际应用中表现非常好，特别是文本分类和垃圾邮件过滤

    缺点:
        条件独立假设在实际中通常不成立，可能导致分类效果不佳
        对于特征之间存在强相关性的情况，朴素贝叶斯的性能可能较差

3.高斯分布(Gaussian Distribution): 也称为正态分布，是统计学中最重要的一种连续概率分布。它以其钟形曲线和许多自然现象中广泛出现的特性而闻名

    高斯分布在许多领域中有广泛应用，包括:
        统计推断: 高斯分布是许多统计方法的基础，如假设检验、置信区间、回归分析等
        信号处理: 在噪声分析中，噪声通常假设为高斯分布
        自然科学: 许多自然现象，如人的身高、考试成绩等，近似服从高斯分布
        金融工程: 股票价格的对数收益常常假设为高斯分布
