1.GRPO（Group Relative Policy Optimization，组相对策略优化）:
    一种基于PPO（Proximal Policy Optimization）改进的强化学习算法，其核心特点如下:
    <1>定义与基础: 通过引入组内相对奖励来估计基线（baseline），取代传统的价值函数模型（如critic model），从而简化训练架构并减少资源消耗
    <2>算法优势: - 舍弃评论家模型（Value Model），通过组分数直接评估策略效果，降低计算复杂度
                - 属于在线学习（online learning），通过迭代使用模型自身生成的数据进行训练，专注于最大化生成内容（如补全任务）的优势
    <3>应用效果: 在DeepSeek V3等场景中验证了其资源效率和性能优化，尤其在增强数学推理能力与内存使用效率方面表现突出，
                与传统PPO的主要区别在于用组计算模块处理多观察值（而非单观察值），并通过相对奖励机制直接优化策略模型，
                省略依赖价值函数估计的环节
