1.vae(Variational AutoEncoder):
    vae是一种强大的生成模型，能够学习数据的潜在(Latent space)分布，从而生成与训练数据相似的新样本。在大型模型中，vae常用于
    生成高质量的图像、文本、音频等复杂数据。例如，在图像生成任务中，vae可以作为生成器的一部分，与其他模型（如生成对抗网络，GAN）结合，
    以提升生成效果和多样性。vae通过编码器将输入数据映射到一个潜在空间，并通过解码器从潜在空间重建数据。这种潜在表示能够捕捉数据的关键特征
    和结构，适用于下游任务。如分类、聚类和回归。在大型模型中，丰富和结构化的潜在空间有助于模型更好地理解和处理复杂的数据关系

2.clip(Contrastive Language–Image Pre-Training):
    CLIP能够在没有特定任务训练的情况下，直接应用于新的任务。例如，可以使用自然语言描述来分类图像，而无需为每个可能的类别进行单独的训练。
    这大大提高了模型的通用性和适应性。通过将图像和文本标签嵌入到相同的向量空间，CLIP可以根据文本描述对图像进行分类。这意味着用户可以使用自定义
    的描述性语言来分类图像，而不仅仅是预定义的标签集。用来实现跨模态的搜索功能。例如，用户可以输入一段描述性的文本，模型能够检索出与之最相关的图像，
    反之亦然。这在内容管理、电子商务和数字资产管理等领域非常有用。CLIP通过联合训练图像和文本，使得模型具备跨模态理解的能力，
    这对于需要综合利用视觉和语言信息的任务（如视觉问答、图文匹配等）非常重要
